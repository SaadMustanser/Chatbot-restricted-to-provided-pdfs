# -*- coding: utf-8 -*-
"""Restricted to given PDFs Chatbot using Vector Databases, Gemini API, and FAISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1vKyITE1dci4m4lA4HXbc8jZxzefgwM
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q PyMuPDF sentence-transformers faiss-cpu google-generativeai

import fitz  # PyMuPDF
import os
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import google.generativeai as genai
import textwrap
from IPython.display import display, Markdown
from google.colab import userdata

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Function to embed text in batches
def embed_pdfs_in_batches(pdf_files, batch_size=50):
    all_chunks = []
    all_chunk_embeddings = []
    chunk_metadata = []

    for pdf_file in pdf_files:
        doc = fitz.open(pdf_file)
        for page_num, page in enumerate(doc):
            text = page.get_text()
            if text.strip():
                all_chunks.append(text)
                chunk_metadata.append((pdf_file, page_num + 1))

        if len(all_chunks) >= batch_size:
            batch_embeddings = embedder.encode(all_chunks, convert_to_tensor=True)
            all_chunk_embeddings.append(np.array(batch_embeddings))
            all_chunks.clear()

    if all_chunks:
        batch_embeddings = embedder.encode(all_chunks, convert_to_tensor=True)
        all_chunk_embeddings.append(np.array(batch_embeddings))

    return chunk_metadata, np.vstack(all_chunk_embeddings), all_chunks

# Define the directory where your PDFs are stored
pdf_directory = "/content/drive/MyDrive/Resumes"

# List all PDF files in the directory
pdf_files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith('.pdf')]

# Process all PDFs in batches
chunk_metadata, all_chunk_embeddings, all_chunks = embed_pdfs_in_batches(pdf_files, batch_size=50)

# Initialize FAISS index
d = all_chunk_embeddings.shape[1]  # Dimension of embeddings
index = faiss.IndexFlatL2(d)
index.add(all_chunk_embeddings)

# Configure the Generative AI API key
GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

# Select the model
model_name = 'gemini-1.5-flash'
model = genai.GenerativeModel(model_name)

def get_gemini_response(prompt, context):
    try:
        combined_prompt = f"Context:\n{context}\n\nQuestion:\n{prompt}"
        response = model.generate_content(combined_prompt)
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

def retrieve_relevant_chunks(query, k=5, threshold=0.75):
    query_embedding = embedder.encode([query], convert_to_tensor=True)
    query_embedding = np.array(query_embedding)

    distances, indices = index.search(query_embedding, k)
    print(f"Distances: {distances}")
    print(f"Indices: {indices}")

    relevant_chunks = []
    relevant_metadata = []

    for i, distance in enumerate(distances[0]):
        if distance <= threshold:
            relevant_chunks.append(all_chunks[indices[0][i]])
            relevant_metadata.append(chunk_metadata[indices[0][i]])

    return relevant_chunks, relevant_metadata

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

def chat():
    print("Chatbot: Hello! How can I help you today?")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("Chatbot: Goodbye!")
            break

        # Retrieve relevant chunks and their metadata
        relevant_chunks, relevant_metadata = retrieve_relevant_chunks(user_input, k=5, threshold=50.0)
        if not relevant_chunks:
            response_text = "I don't have this information. For more information, contact +123456789."
            display(to_markdown(response_text))
            continue

        context = "\n\n".join(relevant_chunks)
        response_text = get_gemini_response(user_input, context)

        if response_text.strip():
            pdf_info = "\n".join([f"The information you are looking for is on page {meta[1]} of the PDF: {os.path.basename(meta[0])}" for meta in relevant_metadata])
            response_text += "\n\n" + pdf_info
        else:
            response_text = "I don't have this information. For more information, contact +123456789."

        display(to_markdown(response_text))

if __name__ == "__main__":
    chat()



